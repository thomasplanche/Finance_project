# -*- coding: utf-8 -*-
"""ch-7-liquidity-modeling.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/gist/thomasplanche/355493b1a0b08c42ea95e03aaf0eee1a/ch-7-liquidity-modeling.ipynb
"""

from google.colab import files
uploaded = files.upload()

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import warnings
warnings.filterwarnings("ignore")
plt.rcParams['figure.figsize'] = (10, 6)
pd.set_option('use_inf_as_na', True)
plt.rcParams['figure.dpi'] = 300
plt.rcParams['savefig.dpi'] = 300

liq_data = pd.read_csv('bid_ask.csv')

"""Affiche les premières lignes de la base de données."""

liq_data.head()

"""Ce code utilise les bibliothèques Matplotlib et Seaborn pour créer un graphique de type violon (violin plot) qui montre la distribution de la variable 'Unnamed: 0' en fonction de chaque valeur unique de 'COMNAM' dans le DataFrame liq_data. Le graphique affiche également un graphique de boîte (boxplot) à l'intérieur de chaque violon. Enfin, il supprime les bords du graphique pour améliorer son apparence."""

# @title COMNAM vs Unnamed: 0

from matplotlib import pyplot as plt
import seaborn as sns
figsize = (12, 1.2 * len(liq_data['COMNAM'].unique()))
plt.figure(figsize=figsize)
sns.violinplot(liq_data, x='Unnamed: 0', y='COMNAM', inner='box', palette='Dark2')
sns.despine(top=True, right=True, bottom=True, left=True)

"""Ce code crée une liste appelée rolling_five et remplit cette liste avec des agrégations de données sur des fenêtres roulantes de cinq jours pour chaque ticker unique dans le DataFrame liq_data"""

rolling_five = []

for j in liq_data.TICKER.unique():
    for i in range(len(liq_data[liq_data.TICKER == j])):
        rolling_five.append(liq_data[i:i+5].agg({'BIDLO': 'min',
                                                'ASKHI': 'max',
                                                 'VOL': 'sum',
                                                 'SHROUT': 'mean',
                                                 'PRC': 'mean'}))

"""Ce code crée un nouveau DataFrame appelé rolling_five_df à partir de la liste rolling_five qui a été remplie précédemment. Ensuite, il renomme les colonnes de ce DataFrame avec les noms appropriés. Enfin, il concatène ce nouveau DataFrame avec le DataFrame liq_data le long de l'axe des colonnes."""

rolling_five_df = pd.DataFrame(rolling_five)
rolling_five_df.columns = ['bidlo_min', 'askhi_max', 'vol_sum',
                           'shrout_mean', 'prc_mean']
liq_vol_all = pd.concat([liq_data,rolling_five_df], axis=1)

"""## Volume Based Measure

Ce code calcule les ratios de liquidité pour chaque ticker unique dans le DataFrame liq_vol_all.
"""

liq_ratio = []

for j in liq_vol_all.TICKER.unique():
    for i in range(len(liq_vol_all[liq_vol_all.TICKER == j])):
        liq_ratio.append((liq_vol_all['PRC'][i+1:i+6] *
                          liq_vol_all['VOL'][i+1:i+6]).sum()/
                         (np.abs(liq_vol_all['PRC'][i+1:i+6].mean() -
                                 liq_vol_all['PRC'][i:i+5].mean())))


print ("Liquidity Ratios:", liq_ratio)

"""Ce code calcule les ratios de Hui-Heubel pour chaque ticker unique dans le DataFrame liq_vol_all"""

Lhh = []

for j in liq_vol_all.TICKER.unique():
    for i in range(len(liq_vol_all[liq_vol_all.TICKER == j])):
        Lhh.append((liq_vol_all['PRC'][i:i+5].max() -
                    liq_vol_all['PRC'][i:i+5].min()) /
                   liq_vol_all['PRC'][i:i+5].min() /
                   (liq_vol_all['VOL'][i:i+5].sum() /
                    liq_vol_all['SHROUT'][i:i+5].mean() *
                    liq_vol_all['PRC'][i:i+5].mean()))

print("Hui-Heubel Ratios:", Lhh)

"""Ce code calcule les ratios de rotation pour chaque ticker dans le DataFrame liq_vol_all en utilisant les volumes et les SHROUT sur une journée, puis stocke ces ratios dans la liste turnover_ratio et les affiche ensuite."""

turnover_ratio = []

for j in liq_vol_all.TICKER.unique():
    for i in range(len(liq_vol_all[liq_vol_all.TICKER == j])):
        turnover_ratio.append((1/liq_vol_all['VOL'].count()) *
                              (np.sum(liq_vol_all['VOL'][i:i+1]) /
                               np.sum(liq_vol_all['SHROUT'][i:i+1])))

print("Turnover Ratios:", turnover_ratio)

"""Ce code ajoute trois nouvelles colonnes contenant les ratios de liquidité, les ratios de Hui-Heubel et les ratios de rotation (turnover) calculés aux données existantes dans le DataFrame liq_vol_all"""

liq_vol_all['liq_ratio'] = pd.DataFrame(liq_ratio)
liq_vol_all['Lhh'] = pd.DataFrame(Lhh)
liq_vol_all['turnover_ratio'] = pd.DataFrame(turnover_ratio)

""" Ce code agrège les données de liq_vol_all par ticker et calcule les moyennes des ratios de liquidité, des ratios de Hui-Heubel et des ratios de rotation, puis affiche le tableau récapitulatif de ces moyennes."""

import pandas as pd
import numpy as np

# Initialiser les listes vides pour stocker les ratios pour IBM
ibm_liq_ratio = []
ibm_Lhh = []
ibm_turnover_ratio = []

# Filtrer les données pour IBM
ibm_data = liq_vol_all[liq_vol_all['TICKER'] == 'IBM']

# Boucler sur les données d'IBM pour calculer les ratios
for i in range(len(ibm_data) - 5):
    # Calculer le ratio de liquidité
    liq_ratio = (ibm_data['PRC'].iloc[i+1:i+6] * ibm_data['VOL'].iloc[i+1:i+6]).sum() / \
                (np.abs(ibm_data['PRC'].iloc[i+1:i+6].mean() - ibm_data['PRC'].iloc[i:i+5].mean()))
    ibm_liq_ratio.append(liq_ratio)

    # Calculer le ratio Hui-Heubel
    Lhh = (ibm_data['PRC'].iloc[i:i+5].max() - ibm_data['PRC'].iloc[i:i+5].min()) / \
          ibm_data['PRC'].iloc[i:i+5].min() / \
          (ibm_data['VOL'].iloc[i:i+5].sum() / (ibm_data['SHROUT'].iloc[i:i+5].mean() * ibm_data['PRC'].iloc[i:i+5].mean()))
    ibm_Lhh.append(Lhh)

    # Calculer le turnover ratio
    turnover_ratio = (1 / ibm_data['VOL'].iloc[i:i+5].count()) * \
                     (ibm_data['VOL'].iloc[i] / ibm_data['SHROUT'].iloc[i])
    ibm_turnover_ratio.append(turnover_ratio)

# Afficher les ratios pour IBM
print("Liquidity Ratios for IBM:", ibm_liq_ratio)
print("Hui-Heubel Ratios for IBM:", ibm_Lhh)
print("Turnover Ratios for IBM:", ibm_turnover_ratio)

# Calculer et afficher la moyenne pour chaque ratio
ibm_liq_ratio_mean = np.mean(ibm_liq_ratio)
ibm_Lhh_mean = np.mean(ibm_Lhh)
ibm_turnover_ratio_mean = np.mean(ibm_turnover_ratio)

print("Mean Liquidity Ratio for IBM:", ibm_liq_ratio_mean)
print("Mean Hui-Heubel Ratio for IBM:", ibm_Lhh_mean)
print("Mean Turnover Ratio for IBM:", ibm_turnover_ratio_mean)

import pandas as pd
import numpy as np

# Initialiser les listes vides pour stocker les ratios pour INTC
intc_liq_ratio = []
intc_Lhh = []
intc_turnover_ratio = []

# Filtrer les données pour INTC
intc_data = liq_vol_all[liq_vol_all['TICKER'] == 'INTC']

# Boucler sur les données d'INTC pour calculer les ratios
for i in range(len(intc_data) - 5):
    # Calculer le ratio de liquidité
    liq_ratio = (intc_data['PRC'].iloc[i+1:i+6] * intc_data['VOL'].iloc[i+1:i+6]).sum() / \
                (np.abs(intc_data['PRC'].iloc[i+1:i+6].mean() - intc_data['PRC'].iloc[i:i+5].mean()))
    intc_liq_ratio.append(liq_ratio)

    # Calculer le ratio Hui-Heubel
    Lhh = (intc_data['PRC'].iloc[i:i+5].max() - intc_data['PRC'].iloc[i:i+5].min()) / \
          intc_data['PRC'].iloc[i:i+5].min() / \
          (intc_data['VOL'].iloc[i:i+5].sum() / (intc_data['SHROUT'].iloc[i:i+5].mean() * intc_data['PRC'].iloc[i:i+5].mean()))
    intc_Lhh.append(Lhh)

    # Calculer le turnover ratio
    turnover_ratio = (1 / intc_data['VOL'].iloc[i:i+5].count()) * \
                     (intc_data['VOL'].iloc[i] / intc_data['SHROUT'].iloc[i])
    intc_turnover_ratio.append(turnover_ratio)

# Afficher les ratios pour INTC
print("Liquidity Ratios for INTC:", intc_liq_ratio)
print("Hui-Heubel Ratios for INTC:", intc_Lhh)
print("Turnover Ratios for INTC:", intc_turnover_ratio)

# Calculer et afficher la moyenne pour chaque ratio
intc_liq_ratio_mean = np.mean(intc_liq_ratio)
intc_Lhh_mean = np.mean(intc_Lhh)
intc_turnover_ratio_mean = np.mean(intc_turnover_ratio)

print("Mean Liquidity Ratio for INTC:", intc_liq_ratio_mean)
print("Mean Hui-Heubel Ratio for INTC:", intc_Lhh_mean)
print("Mean Turnover Ratio for INTC:", intc_turnover_ratio_mean)

import pandas as pd
import numpy as np

# Initialiser les listes vides pour stocker les ratios pour MSFT
msft_liq_ratio = []
msft_Lhh = []
msft_turnover_ratio = []

# Filtrer les données pour MSFT
msft_data = liq_vol_all[liq_vol_all['TICKER'] == 'MSFT']

# Boucler sur les données de MSFT pour calculer les ratios
for i in range(len(msft_data) - 5):
    # Calculer le ratio de liquidité
    liq_ratio = (msft_data['PRC'].iloc[i+1:i+6] * msft_data['VOL'].iloc[i+1:i+6]).sum() / \
                (np.abs(msft_data['PRC'].iloc[i+1:i+6].mean() - msft_data['PRC'].iloc[i:i+5].mean()))
    msft_liq_ratio.append(liq_ratio)

    # Calculer le ratio Hui-Heubel
    Lhh = (msft_data['PRC'].iloc[i:i+5].max() - msft_data['PRC'].iloc[i:i+5].min()) / \
          msft_data['PRC'].iloc[i:i+5].min() / \
          (msft_data['VOL'].iloc[i:i+5].sum() / (msft_data['SHROUT'].iloc[i:i+5].mean() * msft_data['PRC'].iloc[i:i+5].mean()))
    msft_Lhh.append(Lhh)

    # Calculer le turnover ratio
    turnover_ratio = (1 / msft_data['VOL'].iloc[i:i+5].count()) * \
                     (msft_data['VOL'].iloc[i] / msft_data['SHROUT'].iloc[i])
    msft_turnover_ratio.append(turnover_ratio)

# Afficher les ratios pour MSFT
print("Liquidity Ratios for MSFT:", msft_liq_ratio)
print("Hui-Heubel Ratios for MSFT:", msft_Lhh)
print("Turnover Ratios for MSFT:", msft_turnover_ratio)

# Calculer et afficher la moyenne pour chaque ratio
msft_liq_ratio_mean = np.mean(msft_liq_ratio)
msft_Lhh_mean = np.mean(msft_Lhh)
msft_turnover_ratio_mean = np.mean(msft_turnover_ratio)

print("Mean Liquidity Ratio for MSFT:", msft_liq_ratio_mean)
print("Mean Hui-Heubel Ratio for MSFT:", msft_Lhh_mean)
print("Mean Turnover Ratio for MSFT:", msft_turnover_ratio_mean)

"""## Transaction Cost Based Measures

### Bid-Ask Spreads

Ce code calcule le prix médian ainsi que deux pourcentages d'écart (coté et effectif) pour chaque entrée du DataFrame liq_vol_all et les affiche
"""

liq_vol_all['mid_price'] = (liq_vol_all.ASKHI + liq_vol_all.BIDLO) / 2
#prix demandé le plus élevé (ASKHI), prix offert le plus bas (BIDLO), prix médian (mid_price)

#Pourcentage de l'écart coté
liq_vol_all['percent_quoted_ba'] = (liq_vol_all.ASKHI -
                                    liq_vol_all.BIDLO) / \
                                    liq_vol_all.mid_price
#Pourcentage de l'écart effectif
liq_vol_all['percent_effective_ba'] = 2 * abs((liq_vol_all.PRC -
                                               liq_vol_all.mid_price)) / \
                                               liq_vol_all.mid_price

print(liq_vol_all[['mid_price', 'percent_quoted_ba', 'percent_effective_ba']])

"""Ce code trace un graphique comparant les spreads bid-ask coté et effectif pour le ticker IBM, en utilisant les données filtrées du DataFrame liq_vol_all"""

# Filtrage des données pour le ticker IBM
ibm_data = liq_vol_all[liq_vol_all['TICKER'] == 'IBM']

# Création de la figure et des axes
plt.figure(figsize=(10, 6))

# Tracé du spread de Bid-Ask quoted pour IBM en bleu
plt.plot(ibm_data.index, ibm_data['percent_quoted_ba'], label='Quoted Bid-Ask Spread', color='blue')

# Tracé du spread de Bid-Ask effective pour IBM en vert
plt.plot(ibm_data.index, ibm_data['percent_effective_ba'], label='Effective Bid-Ask Spread', color='red')

# Ajout de titre et légendes
plt.title('Comparison of Bid-Ask Spreads for IBM')
plt.xlabel('Index')
plt.ylabel('Spread')
plt.legend()

# Affichage du graphe
plt.grid(True)
plt.tight_layout()
plt.show()

"""### Roll's Spread

Ce code calcule une mesure de volatilité basée sur la covariance des variations de prix sur une fenêtre de cinq jours pour chaque ticker unique dans le DataFrame liq_vol_all, puis stocke ces mesures dans la liste roll
"""

# Convertir la colonne PRC en un type de données approprié (par exemple, float)
liq_vol_all['PRC'] = liq_vol_all['PRC'].astype(float)

# Calculer la différence
liq_vol_all['price_diff'] = liq_vol_all.groupby('TICKER')['PRC'].diff()

# Réinitialiser l'index
liq_vol_all.reset_index(drop=True, inplace=True)

# Supprimer les lignes avec des valeurs manquantes
liq_vol_all.dropna(inplace=True)

# Calculer le spread
roll = []

for j in liq_vol_all['TICKER'].unique():
    for i in range(len(liq_vol_all[liq_vol_all['TICKER'] == j])):
        roll_cov = np.cov(liq_vol_all['price_diff'][i:i+5],
                          liq_vol_all['price_diff'][i+1:i+6])
        if roll_cov[0, 1] < 0:
            roll.append(2 * np.sqrt(-roll_cov[0, 1]))
        else:
            roll.append(2 * np.sqrt(np.abs(roll_cov[0, 1])))

"""Ce code calcule le spread de Roll pour chaque ticker unique dans le DataFrame liq_vol_all en utilisant une fenêtre de cinq jours pour calculer la covariance des variations de prix, puis stocke les résultats dans une nouvelle DataFrame."""

# Création d'une DataFrame pour stocker les résultats
df_roll_spread = pd.DataFrame({
    'Ticker': [],   # Stocke le symbole boursier (ticker)
    'Roll_Spread': []  # Stocke les valeurs du spread de Roll
})

# Parcours de chaque ticker unique
for ticker in liq_vol_all['TICKER'].unique():
    # Récupération des données pour ce ticker
    ticker_data = liq_vol_all[liq_vol_all['TICKER'] == ticker]['price_diff']
    # Parcours des données pour ce ticker avec une fenêtre de cinq jours
    for i in range(len(ticker_data) - 5):
        # Sélection des variations de prix sur une fenêtre de cinq jours
        price_diff_window = ticker_data.iloc[i:i+5]
        # Calcul de la covariance entre les variations de prix sur cette fenêtre de cinq jours
        roll_cov = np.cov(price_diff_window, ticker_data.iloc[i+1:i+6])
        # Vérification si la covariance est négative
        if roll_cov[0, 1] < 0:
            # Calcul du spread de Roll avec une racine carrée positive
            roll_spread = 2 * np.sqrt(-roll_cov[0, 1])
        else:
            # Calcul du spread de Roll avec la valeur absolue de la covariance
            roll_spread = 2 * np.sqrt(np.abs(roll_cov[0, 1]))
        # Ajout du résultat dans la DataFrame
        #df_roll_spread = df_roll_spread.append({'Ticker': ticker, 'Roll_Spread': roll_spread}, ignore_index=True)
        # Ajout du résultat dans la DataFrame df_roll_spread
        df_roll_spread = pd.concat([df_roll_spread, pd.DataFrame({'Ticker': [ticker], 'Roll_Spread': [roll_spread]})], ignore_index=True)

# Affichage de la DataFrame contenant les résultats
print(df_roll_spread)

"""Ce code trace l'évolution du spread de Roll pour le ticker IBM à partir des données fournies dans la DataFrame df_roll_spread, puis affiche le graphique résultant"""

import matplotlib.pyplot as plt

# Filtrer les données pour le ticker IBM
ibm_data = df_roll_spread[df_roll_spread['Ticker'] == 'IBM']

# Tracer le graphique
plt.figure(figsize=(10, 6))
plt.plot(ibm_data['Roll_Spread'], label='Spread de Roll pour IBM')
plt.xlabel('Jours')
plt.ylabel('Spread de Roll')
plt.title('Évolution du spread de Roll pour IBM')
plt.legend()
plt.grid(True)
plt.show()

"""### Corwin and Schultz (2012)

Ce code calcule une mesure de l'écart de prix pour chaque ticker unique dans le DataFrame liq_vol_all et stocke ces valeurs dans un tableau NumPy pour une utilisation ultérieure
"""

gamma = []

for j in liq_vol_all.TICKER.unique():
    for i in range(len(liq_vol_all[liq_vol_all.TICKER == j])):
        gamma.append((max(liq_vol_all['ASKHI'].iloc[i+1],
                          liq_vol_all['ASKHI'].iloc[i]) -
                      min(liq_vol_all['BIDLO'].iloc[i+1],
                          liq_vol_all['BIDLO'].iloc[i])) ** 2)
        gamma_array = np.array(gamma)

"""Ce code calcule une mesure de dispersion des écarts de prix pour chaque ticker unique dans le DataFrame liq_vol_all et stocke ces valeurs dans un tableau NumPy pour une utilisation ultérieure"""

beta = []

for j in liq_vol_all.TICKER.unique():
    for i in range(len(liq_vol_all[liq_vol_all.TICKER == j])):
        beta.append((liq_vol_all['ASKHI'].iloc[i+1] -
                     liq_vol_all['BIDLO'].iloc[i+1]) ** 2 +
                    (liq_vol_all['ASKHI'].iloc[i] -
                     liq_vol_all['BIDLO'].iloc[i]) ** 2)
        beta_array = np.array(beta)

""" Ce code utilise les valeurs de dispersion des écarts de prix (beta_array) et de l'écart de prix (gamma_array) pour calculer une mesure appelée "alpha", puis utilise cette mesure pour calculer le "CS spread" pour chaque ticker unique dans le DataFrame."""

alpha = ((np.sqrt(2 * beta_array) - np.sqrt(beta_array)) /
       (3 - (2 * np.sqrt(2)))) - np.sqrt(gamma_array /
                                         (3 - (2 * np.sqrt(2))))
CS_spread = (2 * np.exp(alpha - 1)) / (1 + np.exp(alpha))

"""Ce code réinitialise l'index du DataFrame liq_vol_all et ajoute deux nouvelles colonnes, 'roll' et 'CS_spread', contenant respectivement les valeurs calculées pour le spread de Roll et le "CS spread"."""

liq_vol_all = liq_vol_all.reset_index()
liq_vol_all['roll'] = pd.DataFrame(roll)
liq_vol_all['CS_spread'] = pd.DataFrame(CS_spread)

"""Ce code trace l'évolution du spread de Corwin-Schultz pour le ticker IBM à partir des données du DataFrame liq_vol_all et affiche le graphique résultant."""

import matplotlib.pyplot as plt

# Sélection des données pour le ticker IBM
ibm_data = liq_vol_all[liq_vol_all['TICKER'] == 'IBM']

# Création du graphique
plt.figure(figsize=(10, 6))
plt.plot(ibm_data.index, ibm_data['CS_spread'], color='blue', linestyle='-')
plt.title('Évolution du spread de Corwin-Schultz pour IBM')
plt.xlabel('Index de l\'observation')
plt.ylabel('Spread de Corwin-Schultz')
plt.grid(True)
plt.show()

"""## Price Based Measures

Ce code calcule le volume de négociation dollarisé pour chaque fenêtre de cinq jours pour chaque ticker unique dans le DataFrame liq_vol_all et ajoute ces valeurs dans une nouvelle colonne appelée 'dvol'. Ensuite, il affiche les valeurs de cette colonne.
"""

dvol = []

for j in liq_vol_all.TICKER.unique():
    for i in range(len(liq_vol_all[liq_vol_all.TICKER == j])):
        dvol.append((liq_vol_all['PRC'][i:i+5] *
                     liq_vol_all['VOL'][i:i+5]).sum())
liq_vol_all['dvol'] = pd.DataFrame(dvol)
print(liq_vol_all['dvol'])

"""Ce code calcule la mesure d'Amihud pour chaque fenêtre de cinq jours pour chaque ticker unique dans le DataFrame liq_vol_all et affiche les valeurs résultantes."""

amihud = []

for j in liq_vol_all.TICKER.unique():
    for i in range(len(liq_vol_all[liq_vol_all.TICKER == j])):
        amihud.append((1 / liq_vol_all['RET'].count()) *
                      (np.sum(np.abs(liq_vol_all['RET'][i:i+1])) /
                              np.sum(liq_vol_all['dvol'][i:i+1])))
print(amihud)

"""Ce code calcule la mesure de Florackis pour chaque fenêtre de cinq jours pour chaque ticker unique dans le DataFrame liq_vol_all."""

florackis = []

for j in liq_vol_all.TICKER.unique():
    for i in range(len(liq_vol_all[liq_vol_all.TICKER == j])):
        florackis.append((1 / liq_vol_all['RET'].count()) *
                         (np.sum(np.abs(liq_vol_all['RET'][i:i+1]) /
                                 liq_vol_all['turnover_ratio'][i:i+1])))
print(florackis)

"""Ce code crée deux nouvelles colonnes dans le DataFrame liq_vol_all : 'vol_diff_pct' pour stocker la variation en pourcentage du volume de transactions et 'price_diff_pct' pour stocker la variation en pourcentage du prix, calculées pour chaque ticker."""

liq_vol_all['VOL'] = liq_vol_all['VOL'].astype(float)
liq_vol_all['vol_diff_pct'] = liq_vol_all.groupby('TICKER')['VOL'].diff()
liq_vol_all['price_diff_pct'] = liq_vol_all.groupby('TICKER')['PRC'].diff()

"""Ce code calcule le CET pour chaque fenêtre de cinq jours pour chaque ticker unique dans le DataFrame liq_vol_all"""

cet = []

for j in liq_vol_all.TICKER.unique():
    for i in range(len(liq_vol_all[liq_vol_all.TICKER == j])):
        cet.append(np.sum(liq_vol_all['vol_diff_pct'][i:i+1])/
                   np.sum(liq_vol_all['price_diff_pct'][i:i+1]))
print(cet)

"""Ce code étend le DataFrame liq_vol_all avec trois nouvelles colonnes contenant les valeurs calculées pour différents indicateurs de liquidité financière."""

liq_vol_all['amihud'] = pd.DataFrame(amihud)
liq_vol_all['florackis'] = pd.DataFrame(florackis)
liq_vol_all['cet'] = pd.DataFrame(cet)

"""## Market Impact Measures"""

import statsmodels.api as sm

"""Ce code calcule la variation en pourcentage du volume de transactions, supprime les lignes avec des valeurs manquantes dans cette colonne, et réinitialise l'index du DataFrame."""

liq_vol_all['VOL_pct_change'] = liq_vol_all.groupby('TICKER')['VOL'].pct_change()
liq_vol_all.dropna(subset=['VOL_pct_change'], inplace=True)
liq_vol_all = liq_vol_all.reset_index()

"""Ce code calcule les résidus des régressions linéaires entre les rendements des différents tickers et le rendement du marché."""

unsys_resid = []

for i in liq_vol_all.TICKER.unique():
    X1 = liq_vol_all[liq_vol_all['TICKER'] == i]['vwretx']
    y = liq_vol_all[liq_vol_all['TICKER'] == i]['RET']
    ols = sm.OLS(y, X1).fit()
    unsys_resid.append(ols.resid)

"""Ce code semble calculer l'impact sur le marché pour chaque ticker unique."""

market_impact = {}

for i, j in zip(liq_vol_all.TICKER.unique(),
                range(len(liq_vol_all['TICKER'].unique()))):
    X2 = liq_vol_all[liq_vol_all['TICKER'] == i]['VOL_pct_change']
    ols = sm.OLS(unsys_resid[j] ** 2, X2).fit()
    print('***' * 30)
    print(f'OLS Result for {i}')
    print(ols.summary())
    market_impact[j] = ols.resid

"""Ce code semble fusionner les séries temporelles des résidus d'impact sur le marché obtenues à partir des régressions linéaires effectuées précédemment pour chaque ticker unique."""

append1 = pd.concat([market_impact[0], market_impact[1]])
append1 = pd.concat([append1, market_impact[2]])
# Assignation de la nouvelle Series à la colonne 'market_impact' de la DataFrame `liq_vol_all`
liq_vol_all['market_impact'] = append1

"""liq_vol_all.drop(liq_vol_all[cols], axis=1) supprime les colonnes spécifiées dans la liste cols du DataFrame liq_vol_all.

.iloc[:, -11:] sélectionne les 11 dernières colonnes restantes après suppression des colonnes spécifiées.

liq_measures_all.dropna(inplace=True) supprime toutes les lignes contenant des valeurs manquantes dans le DataFrame résultant.

Enfin, .describe().T génère un résumé statistique des données dans le DataFrame transposé, fournissant des statistiques descriptives telles que la moyenne, l'écart type, les quantiles, etc.
"""

cols = ['vol_diff_pct', 'price_diff_pct', 'price_diff',
        'VOL_pct_change', 'dvol', 'mid_price']
liq_measures_all = liq_vol_all.drop(liq_vol_all[cols], axis=1)\
                   .iloc[:, -11:]
liq_measures_all.dropna(inplace=True)
liq_measures_all.describe().T

"""## GMM"""

from sklearn.mixture import GaussianMixture
from sklearn.preprocessing import StandardScaler

"""liq_measures_all.dropna() supprime toutes les lignes contenant des valeurs manquantes dans le DataFrame liq_measures_all. Cela garantit que nous travaillons avec un DataFrame complet sans valeurs manquantes.

StandardScaler().fit_transform(liq_measures_all2) standardise les données dans le DataFrame liq_measures_all2. La standardisation implique de centrer les données autour de zéro (en soustrayant la moyenne) et de les mettre à l'échelle en divisant par l'écart type. Cela permet de rendre les données comparables et facilite l'analyse des modèles prédictifs ou des algorithmes d'apprentissage automatique.
"""

liq_measures_all2 = liq_measures_all.dropna()
scaled_liq = StandardScaler().fit_transform(liq_measures_all2)

"""Ce code crée un histogramme qui compare la distribution de trois mesures de liquidité différentes (percent_quoted_ba, turnover_ratio, et market_impact) présentes dans le DataFrame liq_measures_all."""

kwargs = dict(alpha=0.5, bins=50,  stacked=True)
plt.hist(liq_measures_all.loc[:, 'percent_quoted_ba'],
         **kwargs, label='TC-based')
plt.hist(liq_measures_all.loc[:, 'turnover_ratio'],
         **kwargs, label='Volume-based')
plt.hist(liq_measures_all.loc[:, 'market_impact'],
         **kwargs, label='Market-based')
plt.title('Multimodality of the Liquidity Measures')
plt.legend()
plt.show()

"""Ce code effectue une analyse pour déterminer le nombre optimal de composants (clusters) à utiliser dans un modèle de mélange gaussien"""

n_components = np.arange(1, 10)
clusters = [GaussianMixture(n, covariance_type='spherical',
                            random_state=0).fit(scaled_liq)
          for n in n_components]
plt.plot(n_components, [m.bic(scaled_liq) for m in clusters])
plt.title('Optimum Number of Components')
plt.xlabel('n_components')
plt.ylabel('BIC values')
plt.show()

"""
Ce code définit une fonction cluster_state qui utilise un modèle de mélange gaussien  pour effectuer une analyse de clustering sur les données passées en argument. La fonction prend également le nombre d'états comme paramètre pour spécifier le nombre de clusters souhaité.La fonction renvoie les probabilités d'appartenance à chaque état pour chaque échantillon."""

def cluster_state(data, nstates):
    gmm = GaussianMixture(n_components=nstates,
                          covariance_type='spherical',
                          init_params='kmeans')
    gmm_fit = gmm.fit(scaled_liq)
    labels = gmm_fit.predict(scaled_liq)
    state_probs = gmm.predict_proba(scaled_liq)
    state_probs_df = pd.DataFrame(state_probs,
                                  columns=['state-1','state-2','state-3'])
    state_prob_means = [state_probs_df.iloc[:, i].mean()
                        for i in range(len(state_probs_df.columns))]
    if np.max(state_prob_means) == state_prob_means[0]:
        print('State-1 is likely to occur with a probability of {:4f}'
              .format(state_prob_means[0]))
    elif np.max(state_prob_means) == state_prob_means[1]:
        print('State-2 is likely to occur with a probability of {:4f}'
              .format(state_prob_means[1]))
    else:
        print('State-3 is likely to occur with a probability of {:4f}'
              .format(state_prob_means[2]))
    return state_probs

"""Ce code utilise la fonction cluster_state pour effectuer une analyse de clustering sur les données scaled_liq avec un nombre de clusters spécifié de 3. Ensuite, il calcule les probabilités moyennes d'appartenance à chaque état (cluster) à partir des probabilités renvoyées par la fonction cluster_state, en prenant la moyenne le long de l'axe des échantillons. Enfin, il affiche les probabilités moyennes pour chaque état."""

state_probs = cluster_state(scaled_liq, 3)
print(f'State probabilities are {state_probs.mean(axis=0)}')

from sklearn.decomposition import PCA

"""Ce code utilise l'analyse en composantes principales (PCA) pour transformer les données scaled_liq en un nouvel espace de variables décorrélées appelées composantes principales. Ensuite, il trace le graphique de l'importance de chaque composante principale en termes de pourcentage de variance expliquée. Ce graphique est souvent appelé un "Scree Plot" et est utilisé pour déterminer le nombre optimal de composantes principales à conserver."""

pca = PCA(n_components=11)
components = pca.fit_transform(scaled_liq)
plt.plot(pca.explained_variance_ratio_)
plt.title('Scree Plot')
plt.xlabel('Number of Components')
plt.ylabel('% of Explained Variance')
plt.show()

"""Ce code effectue une analyse en composantes principales (PCA) sur les données data, en réduisant les dimensions à 3 composantes principales. Ensuite, il applique un modèle de mélange gaussien (GMM) avec nstate états cachés sur ces composantes principales. Le modèle GMM est ajusté aux données réduites, puis les probabilités d'appartenance à chaque état caché sont calculées à l'aide de la méthode predict_proba. Finalement, il retourne à la fois les probabilités des états cachés et l'objet PCA pour permettre une analyse plus approfondie si nécessaire."""

def gmm_pca(data, nstate):
    pca = PCA(n_components=3)
    components = pca.fit_transform(data)
    mxtd = GaussianMixture(n_components=nstate,
                           covariance_type='spherical')
    gmm = mxtd.fit(components)
    labels = gmm.predict(components)
    state_probs = gmm.predict_proba(components)
    return state_probs,pca

"""Ce code applique la fonction gmm_pca aux données scaled_liq avec nstate égal à 3. Ensuite, il affiche les probabilités moyennes des états cachés en prenant la moyenne le long de l'axe 0 de la matrice state_probs. Ces probabilités moyennes indiquent la probabilité moyenne d'observation appartenant à chaque état caché."""

state_probs, pca = gmm_pca(scaled_liq, 3)
print(f'State probabilities are {state_probs.mean(axis=0)}')

"""Ce code définit une fonction wpc qui calcule les probabilités moyennes des états cachés à partir de la matrice state_probs et affiche ensuite l'état le plus probable avec sa probabilité moyenne. Ensuite, il appelle cette fonction."""

def wpc():
    state_probs_df = pd.DataFrame(state_probs,
                                  columns=['state-1', 'state-2',
                                           'state-3'])
    state_prob_means = [state_probs_df.iloc[:, i].mean()
                        for i in range(len(state_probs_df.columns))]
    if np.max(state_prob_means) == state_prob_means[0]:
        print('State-1 is likely to occur with a probability of {:4f}'
              .format(state_prob_means[0]))
    elif np.max(state_prob_means) == state_prob_means[1]:
        print('State-2 is likely to occur with a probability of {:4f}'
              .format(state_prob_means[1]))
    else:
        print('State-3 is likely to occur with a probability of {:4f}'
              .format(state_prob_means[2]))
wpc()

"""Ce code calcule la matrice de chargement (loading matrix) à partir des composantes principales (principal components) obtenues après l'analyse en composantes principales (PCA). La matrice de chargement est calculée en multipliant les composantes principales par la racine carrée des valeurs propres correspondantes (explained variance). Ensuite, il crée un DataFrame contenant cette matrice de chargement, avec les noms des variables d'entrée en tant qu'index et les noms des composantes principales (PC1, PC2, PC3) en tant que colonnes."""

loadings = pca.components_.T * np.sqrt(pca.explained_variance_)
loading_matrix = pd.DataFrame(loadings,
                              columns=['PC1', 'PC2', 'PC3'],
                              index=liq_measures_all.columns)
loading_matrix

"""## GMCM"""

!pip install copulae

from copulae.mixtures.gmc.gmc import GaussianMixtureCopula

_, dim = scaled_liq.shape
gmcm = GaussianMixtureCopula(n_clusters=3, ndim=dim)

"""Ce code utilise le modèle de mélange gaussien pour les données pour ajuster les données mises à l'échelle scaled_liq. Il utilise la méthode de clustering K-means avec le critère GMCM et une tolérance (eps) de 0.0001. Ensuite, il extrait les probabilités d'appartenance à chaque état à partir des paramètres ajustés du modèle et imprime le numéro de l'état le plus probable ainsi que les probabilités d'appartenance à chaque état."""

gmcm_fit = gmcm.fit(scaled_liq, method='kmeans',
                    criteria='GMCM', eps=0.0001)
state_prob = gmcm_fit.params.prob
print(f'The state {np.argmax(state_prob) + 1} is likely to occur')
print(f'State probabilities based on GMCM are {state_prob}')